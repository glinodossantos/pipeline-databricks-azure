<h1 align="center">Projeto de Criação de Pipelines nas Nuvens usando Datalake, Databricks e Data Factory</h1>
  
</p>

<h2><img src="https://cdn-icons-png.flaticon.com/128/7235/7235711.png" width=22> Descrição do Projeto</h2>

<p>
  Este projeto foi desenvolvido como parte do curso Alura, ministrado pela instrutora Millena Gená Pereira, de criação de pipelines nas nuvens usando tecnologias como Datalake, Databricks e Data Factory. O objetivo do projeto é demonstrar o fluxo de dados entre diferentes serviços em um ambiente de nuvem, permitindo o processamento e transformação de grandes volumes de dados de forma eficiente e escalável.
</p>

<h2><img src="https://cdn-icons-png.flaticon.com/128/4698/4698123.png" width=22> Funcionalidades</h2>

<ol>
  <li>
    <strong>Construção de um Pipeline de Engenharia de Dados</strong>: O projeto tem como objetivo guiar o usuário na construção de um pipeline de Engenharia de Dados, abrangendo todas as etapas necessárias para coleta, processamento e disponibilização dos dados.
  </li>
  <li>
    <strong>Estruturação do Data Lake com Azure Data Lake Storage Gen 2</strong>: Será ensinado como criar e estruturar um Data Lake utilizando o serviço Azure Data Lake Storage Gen 2. O Data Lake oferece um ambiente escalável e seguro para armazenar e organizar dados brutos e processados.
  </li>
  <li>
    <strong>Configuração do Databricks com Serviço de Cloud da Azure</strong>: Os usuários aprenderão a configurar o Databricks, plataforma de análise e processamento de Big Data da Azure, conectando-o ao serviço de Cloud da Azure para aproveitar os recursos de nuvem.
  </li>
  <li>
    <strong>Desenvolvimento de Notebooks em Databricks com a Linguagem Scala</strong>: Utilizando a linguagem Scala, os usuários irão desenvolver notebooks no ambiente Databricks, permitindo a execução de código Spark para análise e manipulação de dados em larga escala.
  </li>
  <li>
    <strong>Construção de Pipelines com Azure Data Factory</strong>: Será abordado como construir pipelines utilizando o serviço Azure Data Factory, permitindo a orquestração das atividades de coleta, processamento e transformação dos dados.
  </li>
  <li>
    <strong>Integração com o GitHub</strong>: O projeto irá demonstrar como integrar todo o código e artefatos do projeto de Engenharia de Dados com o GitHub, permitindo o versionamento e controle de alterações.
  </li>
  <li>
    <strong>Definição de Gatilhos de Execução</strong>: Os usuários aprenderão a definir gatilhos de execução no Azure Data Factory, permitindo a automatização do pipeline em resposta a eventos específicos, como novos dados disponíveis.
  </li>
  <li>
    <strong>Colocação do Pipeline em Produção</strong>: O projeto ensinará como colocar o pipeline de Engenharia de Dados em produção, garantindo que o fluxo de dados ocorra de forma confiável e consistente.
  </li>
</ol>

<h2><img src="https://cdn-icons-png.flaticon.com/128/3715/3715323.png" width=22> Tecnologias Utilizadas</h2>

<ul>
  <li>
    Microsoft Azure: O projeto foi desenvolvido na plataforma de nuvem da Microsoft, utilizando serviços como Azure Datalake Storage, Azure Databricks e Azure Data Factory.
  </li>
  <li>
    Azure Datalake Storage: É o serviço de armazenamento em nuvem altamente escalável, seguro e de baixo custo, ideal para armazenar grandes volumes de dados brutos.
  </li>
  <li>
    Azure Databricks: É uma plataforma de análise e processamento de Big Data com Apache Spark. Oferece um ambiente colaborativo e interativo para executar código Spark e realizar análises sofisticadas em larga escala.
  </li>
  <li>
    Azure Data Factory: É um serviço de orquestração de dados que permite criar, agendar e gerenciar pipelines de dados em escala empresarial. Ele oferece recursos para transformar e mover dados entre diferentes fontes e destinos.
  </li>
</ul>

<h2><img src="https://cdn-icons-png.flaticon.com/128/831/831629.png" width=22> Instruções de Uso</h2>

<ol>
  <li>
    Clone este repositório em sua máquina local.
  </li>
  <li>
    Acesse a plataforma do Microsoft Azure e crie os recursos necessários:
    <ul>
      <li>Crie um novo Datalake Storage para armazenar os dados brutos.</li>
      <li>Provisione uma instância do Databricks para executar o processamento dos dados.</li>
      <li>Crie uma instância do Data Factory para orquestrar o pipeline.</li>
    </ul>
  </li>
  <li>
    Configure as credenciais e conexões necessárias nos arquivos de configuração do projeto.
  </li>
  <li>
    Importe os notebooks do Databricks contidos neste repositório para a instância do Databricks.
  </li>
  <li>
    Crie o pipeline na Data Factory e agende sua execução conforme a necessidade.
  </li>
  <li>
    Execute o pipeline e acompanhe o monitoramento para verificar o fluxo de dados e o processamento realizado.
  </li>
</ol>
